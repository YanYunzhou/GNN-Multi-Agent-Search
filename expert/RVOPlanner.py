"""
Example file showing a demo with 100 agents split in four groups initially positioned in four corners of the environment. Each agent attempts to move to other side of the environment through a narrow passage generated by four obstacles. There is no roadmap to guide the agents around the obstacles.
"""
import math
import random
import re
# import gym.envs.classic_control.rendering as rendering

import os
import sys
import numpy as np
currentdir = os.path.dirname(os.path.realpath(__file__))

parentdir = os.path.dirname(currentdir)
sys.path.append(parentdir)

import expert.rvo.math as rvo_math

from expert.rvo.vector import Vector2
from expert.rvo.simulator import Simulator

RVO_RENDER = True

## implementation of the RVO algorithm:
#  https://github.com/jimfleming/rvo2

class RVOPlanner:

    def __init__(self):
        self.goals_ = [] # Vector2
        self.obstacles_ = [] # Vector2
        self.simulator_ = Simulator()

    def setup_scenario(self, env, starts, goals, stepsize):
        # Specify the global time step of the simulation.
        # self.simulator_.set_time_step(1/30)
        self.simulator_.set_time_step(stepsize)

        # Specify the default parameters for agents that are subsequently added.
        # self.simulator_.set_agent_defaults(15.0, 10, 5.0, 5.0, 2.0, 2.0, Vector2(0.0, 0.0))

        # Add agents, specifying their start position, and store their goals on the opposite side of the environment.
        env.reset(starts, goals)
        self.MIN_THRESHOLD = env.MIN_THRESHOLD
        self.max_speed = env.median_edge_weight
        self.simulator_.set_agent_defaults(neighborDist=env.MIN_THRESHOLD, maxNeighbors=5, \
                                            timeHorizon=2, timeHorizonObst=2, \
                                            radius=env.RAD_DEFAULT, maxSpeed=float(env.median_edge_weight), 
                                            velocity=Vector2(0.0, 0.0))

        for id_agent, pos in env.agents_cur_pos.items():
            self.simulator_.add_agent(Vector2(pos[0], pos[1]))
            self.goals_.append(Vector2(env.agents_goal_pos[id_agent][0], env.agents_goal_pos[id_agent][1]))


        # Add (polygonal) obstacles, specifying their vertices in counterclockwise order.
        if env.config.env_name == 'BoxEnv':
            for box in (env.border_boxes+env.obstacle_boxes):
                obstacle = []
                for item in box['shape'].exterior.coords[:-1]:
                    obstacle.append(Vector2(item[0]*env.scale, item[1]*env.scale))
                self.simulator_.add_obstacle(obstacle)
                self.obstacles_.append(obstacle)
            
            
        elif env.config.env_name == 'MazeEnv':
            for i in range(env.map.shape[0]):
                for j in range(env.map.shape[1]):
                    if env.map[i, j] == 1:
                        width = (1 / env.width)#*0.9
                        pos = [env._inverse_transform([i, j])[0]+(1 / env.width), env._inverse_transform([i, j])[1]+(1 / env.width)]
                        # obstacle = [Vector2(pos[0]-width, pos[1]-width), Vector2(pos[0]-width, pos[1]+width), Vector2(pos[0]+width, pos[1]+width), Vector2(pos[0]+width, pos[1]-width)]
                        obstacle = [Vector2(pos[0]-width, pos[1]+width), Vector2(pos[0]-width, pos[1]-width), Vector2(pos[0]+width, pos[1]-width), Vector2(pos[0]+width, pos[1]+width)]

                        self.simulator_.add_obstacle(obstacle)
                        self.obstacles_.append(obstacle)

            
        # Process the obstacles so that they are accounted for in the simulation.
        self.simulator_.process_obstacles()

    # def update_visualization(self, viewer):
    #     if not RVO_RENDER:
    #         return

    #     for i in range(self.simulator_.num_agents):
    #         position = self.simulator_.agents_[i].position_
    #         color = [0, 0, 0]
    #         color[i % 3] = 1
    #         circle = viewer.draw_circle(radius=self.simulator_.default_agent_.radius_, color=color)
    #         circle.add_attr(rendering.Transform(translation=(position.x, position.y)))

    #     for obstacle in self.obstacles_:
    #         v = [(vec.x, vec.y) for vec in obstacle]
    #         viewer.draw_polygon(v=v, color=(0, 0, 0))

    #     viewer.render()

    def set_preferred_velocities(self):
        # Set the preferred velocity to be a vector of unit magnitude (speed) in the direction of the goal.
        v = []
        for i in range(self.simulator_.num_agents):
            goal_vector = self.goals_[i] - self.simulator_.agents_[i].position_

            if rvo_math.abs_sq(goal_vector) > 1.0:
                goal_vector = rvo_math.normalize(goal_vector)

            self.simulator_.set_agent_pref_velocity(i, goal_vector)

            # Perturb a little to avoid deadlocks due to perfect symmetry.
            angle = random.random() * 2.0 * math.pi
            dist = random.random() * 0.01#0.0001

            v_pref =  self.simulator_.agents_[i].pref_velocity_ + dist * Vector2(math.cos(angle), math.sin(angle))
            
            norm_v = np.sqrt(v_pref.x**2 + v_pref.y**2)
            if norm_v > self.max_speed:
                v_pref = rvo_math.normalize(v_pref)* self.max_speed
            
            v.append([v_pref.x, v_pref.y])
            self.simulator_.set_agent_pref_velocity(i, v_pref)
        
    def get_states(self):
        states = []
        for i in range(self.simulator_.num_agents):
            states.append([self.simulator_.agents_[i].position_.x, self.simulator_.agents_[i].position_.y])
        return states

    def reached_goal(self):
        # Check if all agents have reached their goals.
        agent_reached_goal = [False] * self.simulator_.num_agents
        for i in range(self.simulator_.num_agents):
            if rvo_math.abs_sq(self.simulator_.agents_[i].position_ - self.goals_[i]) < self.MIN_THRESHOLD:
                agent_reached_goal[i] = True
        
        return all(agent_reached_goal)

